{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the libraries\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "from time import time\n",
    "from torchvision import datasets, transforms\n",
    "from torch import nn, optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3.1\n"
     ]
    }
   ],
   "source": [
    "# version of pytorch\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformations to be applied on images\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                              transforms.Normalize((0.5,), (0.5,)),\n",
    "                              ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Now, letâ€™s load the training and testing set of the MNIST dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data\\MNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9920512it [00:03, 2600261.90it/s]                                                                             \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\train-images-idx3-ubyte.gz to ./data\\MNIST\\raw\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "32768it [00:00, 35280.96it/s]                                                                                 \n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\train-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1654784it [00:02, 713469.96it/s]                                                                              \n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz to ./data\\MNIST\\raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8192it [00:00, 11923.13it/s]                                                                                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\n",
      "Processing...\n",
      "Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./MNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9920512it [00:12, 1047958.09it/s]                                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./MNIST\\raw\\train-images-idx3-ubyte.gz to ./MNIST\\raw\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0it [00:00, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./MNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "32768it [00:00, 35995.69it/s]                                                                                 \u001b[A\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./MNIST\\raw\\train-labels-idx1-ubyte.gz to ./MNIST\\raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./MNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "1654784it [00:02, 703034.84it/s]                                                                              \u001b[A\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./MNIST\\raw\\t10k-images-idx3-ubyte.gz to ./MNIST\\raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "8192it [00:00, 11880.42it/s]                                                                                  \u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./MNIST\\raw\\t10k-labels-idx1-ubyte.gz to ./MNIST\\raw\n",
      "Processing...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# defining the training and testing set\n",
    "trainset = datasets.MNIST('./data', download=True, train=True, transform=transform)\n",
    "testset = datasets.MNIST('./', download=True, train=False, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "9920512it [00:29, 1047958.09it/s]"
     ]
    }
   ],
   "source": [
    "# Next, I have defined the train and test loader which will help us to load the training and test set in batches. I will define the batch size as 64:\n",
    "\n",
    "# defining trainloader and testloader\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "# shape of training data\n",
    "dataiter = iter(trainloader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "print(images.shape)\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1adc9c62f60>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# So, in each batch, we have 64 images, each of size 28,28 and for each image, we have a corresponding label. Letâ€™s visualize a training image and see how it looks:\n",
    "\n",
    "# visualizing the training images\n",
    "plt.imshow(images[0].numpy().squeeze(), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "#Itâ€™s an image of number 0. Similarly, letâ€™s visualize the test set image:\n",
    "\n",
    "# shape of validation data\n",
    "dataiter = iter(testloader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "print(images.shape)\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining Model Architecture\n",
    "# We will be using a CNN model here. So let us define and train this model:\n",
    "\n",
    "# defining the model architecture\n",
    "class Net(nn.Module):   \n",
    "  def __init__(self):\n",
    "      super(Net, self).__init__()\n",
    "\n",
    "      self.cnn_layers = nn.Sequential(\n",
    "          # Defining a 2D convolution layer\n",
    "          nn.Conv2d(1, 4, kernel_size=3, stride=1, padding=1),\n",
    "          nn.BatchNorm2d(4),\n",
    "          nn.ReLU(inplace=True),\n",
    "          nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "          # Defining another 2D convolution layer\n",
    "          nn.Conv2d(4, 4, kernel_size=3, stride=1, padding=1),\n",
    "          nn.BatchNorm2d(4),\n",
    "          nn.ReLU(inplace=True),\n",
    "          nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "      )\n",
    "\n",
    "      self.linear_layers = nn.Sequential(\n",
    "          nn.Linear(4 * 7 * 7, 10)\n",
    "      )\n",
    "\n",
    "  # Defining the forward pass    \n",
    "  def forward(self, x):\n",
    "      x = self.cnn_layers(x)\n",
    "      x = x.view(x.size(0), -1)\n",
    "      x = self.linear_layers(x)\n",
    "      return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Letâ€™s also define the optimizer and loss function then we will look at the summary of this model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (cnn_layers): Sequential(\n",
      "    (0): Conv2d(1, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (4): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (5): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (6): ReLU(inplace=True)\n",
      "    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (linear_layers): Sequential(\n",
      "    (0): Linear(in_features=196, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# defining the model\n",
    "model = Net()\n",
    "# defining the optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "# defining the loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# checking if GPU is available\n",
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()\n",
    "    criterion = criterion.cuda()\n",
    "    \n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we have 2 convolutional layers that will help to extract features from the images. Features from these convolutional layers are passed to the fully connected layer which classifies the images into their respective class. Now our model architecture is ready, letâ€™s train this model for 10 epochs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Training loss: 0.16605456616046396\n",
      "Epoch 2 - Training loss: 0.09048631871148928\n",
      "Epoch 3 - Training loss: 0.07914899759141526\n",
      "Epoch 4 - Training loss: 0.07150491072596717\n",
      "Epoch 5 - Training loss: 0.06794889853261452\n",
      "Epoch 6 - Training loss: 0.06573310087049958\n",
      "Epoch 7 - Training loss: 0.06363410825184834\n",
      "Epoch 8 - Training loss: 0.061638841598502185\n",
      "Epoch 9 - Training loss: 0.05831823686533955\n",
      "Epoch 10 - Training loss: 0.05911459443050184\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    running_loss = 0\n",
    "    for images, labels in trainloader:\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "          images = images.cuda()\n",
    "          labels = labels.cuda()\n",
    "\n",
    "        # Training pass\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model(images)\n",
    "        loss = criterion(output, labels)\n",
    "        \n",
    "        #This is where the model learns by backpropagating\n",
    "        loss.backward()\n",
    "        \n",
    "        #And optimizes its weights here\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    else:\n",
    "        print(\"Epoch {} - Training loss: {}\".format(i+1, running_loss/len(trainloader)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that the training is decreasing with an increasing number of epochs. This means that our model is learning patterns from the training set. Letâ€™s check the performance of this model on the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number Of Images Tested = 10000\n",
      "\n",
      "Model Accuracy = 0.9629\n"
     ]
    }
   ],
   "source": [
    "# getting predictions on test set and measuring the performance\n",
    "correct_count, all_count = 0, 0\n",
    "for images,labels in testloader:\n",
    "  for i in range(len(labels)):\n",
    "    if torch.cuda.is_available():\n",
    "        images = images.cuda()\n",
    "        labels = labels.cuda()\n",
    "    img = images[i].view(1, 1, 28, 28)\n",
    "    with torch.no_grad():\n",
    "        logps = model(img)\n",
    "\n",
    "    \n",
    "    ps = torch.exp(logps)\n",
    "    probab = list(ps.cpu()[0])\n",
    "    pred_label = probab.index(max(probab))\n",
    "    true_label = labels.cpu()[i]\n",
    "    if(true_label == pred_label):\n",
    "      correct_count += 1\n",
    "    all_count += 1\n",
    "\n",
    "print(\"Number Of Images Tested =\", all_count)\n",
    "print(\"\\nModel Accuracy =\", (correct_count/all_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing a CNN in TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bsram\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\bsram\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\bsram\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\bsram\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\bsram\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\bsram\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "C:\\Users\\bsram\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\bsram\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\bsram\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\bsram\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\bsram\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\bsram\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "# importing the libraries\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# version of tensorflow\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we are using the 2.2.0 version of TensorFlow. Letâ€™s now load the MNIST dataset using the datasets class of tensorflow.keras:\n",
    "\n",
    "(train_images, train_labels), (test_images, test_labels) = datasets.mnist.load_data(path='mnist.npz')\n",
    "# Normalize pixel values to be between 0 and 1\n",
    "train_images, test_images = train_images / 255.0, test_images / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualizing a few images\n",
    "plt.figure(figsize=(10,10))\n",
    "for i in range(9):\n",
    "    plt.subplot(3,3,i+1)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.grid(False)\n",
    "    plt.imshow(train_images[i], cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shape of the training and test set\n",
    "(train_images.shape, train_labels.shape), (test_images.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshaping the images\n",
    "train_images = train_images.reshape((60000, 28, 28, 1))\n",
    "test_images = test_images.reshape((10000, 28, 28, 1))\n",
    "\n",
    "# one hot encoding the target variable\n",
    "train_labels = to_categorical(train_labels)\n",
    "test_labels = to_categorical(test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Model Architecture\n",
    "Now, we will define the architecture of our model. We will use the same architecture which we defined in PyTorch. So, our model will have 2 convolutional layers, with a combination of max-pooling layers, then we will have a flatten layer and finally a dense layer with 10 neurons since we have 10 classes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# defining the model architecture\n",
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(4, (3, 3), activation='relu', input_shape=(28, 28, 1)))\n",
    "model.add(layers.MaxPooling2D((2, 2), strides=2))\n",
    "model.add(layers.Conv2D(4, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2), strides=2))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Letâ€™s quickly look at the summary of the model:\n",
    "\n",
    "# summary of the model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compiling the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training the model\n",
    "history = model.fit(train_images, train_labels, epochs=10, validation_data=(test_images, test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes\n",
    "To summarize,we first looked at a brief overview of PyTorch and TensorFlow. Then we understood the MNIST handwritten digit classification challenge and finally, build an image classification model using CNN(Convolutional Neural Network) in PyTorch and TensorFlow. Now, I hope you will be familiar with both these frameworks. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
