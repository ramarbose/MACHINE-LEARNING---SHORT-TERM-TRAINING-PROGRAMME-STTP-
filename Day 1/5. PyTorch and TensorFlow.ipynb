{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the libraries\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "from time import time\n",
    "from torchvision import datasets, transforms\n",
    "from torch import nn, optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3.1\n"
     ]
    }
   ],
   "source": [
    "# version of pytorch\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformations to be applied on images\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                              transforms.Normalize((0.5,), (0.5,)),\n",
    "                              ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Now, let’s load the training and testing set of the MNIST dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data\\MNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9920512it [00:03, 2600261.90it/s]                                                                             \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\train-images-idx3-ubyte.gz to ./data\\MNIST\\raw\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "32768it [00:00, 35280.96it/s]                                                                                 \n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\train-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1654784it [00:02, 713469.96it/s]                                                                              \n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz to ./data\\MNIST\\raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8192it [00:00, 11923.13it/s]                                                                                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\n",
      "Processing...\n",
      "Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./MNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9920512it [00:12, 1047958.09it/s]                                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./MNIST\\raw\\train-images-idx3-ubyte.gz to ./MNIST\\raw\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0it [00:00, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./MNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "32768it [00:00, 35995.69it/s]                                                                                 \u001b[A\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./MNIST\\raw\\train-labels-idx1-ubyte.gz to ./MNIST\\raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./MNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "1654784it [00:02, 703034.84it/s]                                                                              \u001b[A\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./MNIST\\raw\\t10k-images-idx3-ubyte.gz to ./MNIST\\raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "8192it [00:00, 11880.42it/s]                                                                                  \u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./MNIST\\raw\\t10k-labels-idx1-ubyte.gz to ./MNIST\\raw\n",
      "Processing...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# defining the training and testing set\n",
    "trainset = datasets.MNIST('./data', download=True, train=True, transform=transform)\n",
    "testset = datasets.MNIST('./', download=True, train=False, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "9920512it [00:29, 1047958.09it/s]"
     ]
    }
   ],
   "source": [
    "# Next, I have defined the train and test loader which will help us to load the training and test set in batches. I will define the batch size as 64:\n",
    "\n",
    "# defining trainloader and testloader\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "# shape of training data\n",
    "dataiter = iter(trainloader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "print(images.shape)\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1adc9c62f60>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# So, in each batch, we have 64 images, each of size 28,28 and for each image, we have a corresponding label. Let’s visualize a training image and see how it looks:\n",
    "\n",
    "# visualizing the training images\n",
    "plt.imshow(images[0].numpy().squeeze(), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "#It’s an image of number 0. Similarly, let’s visualize the test set image:\n",
    "\n",
    "# shape of validation data\n",
    "dataiter = iter(testloader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "print(images.shape)\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining Model Architecture\n",
    "# We will be using a CNN model here. So let us define and train this model:\n",
    "\n",
    "# defining the model architecture\n",
    "class Net(nn.Module):   \n",
    "  def __init__(self):\n",
    "      super(Net, self).__init__()\n",
    "\n",
    "      self.cnn_layers = nn.Sequential(\n",
    "          # Defining a 2D convolution layer\n",
    "          nn.Conv2d(1, 4, kernel_size=3, stride=1, padding=1),\n",
    "          nn.BatchNorm2d(4),\n",
    "          nn.ReLU(inplace=True),\n",
    "          nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "          # Defining another 2D convolution layer\n",
    "          nn.Conv2d(4, 4, kernel_size=3, stride=1, padding=1),\n",
    "          nn.BatchNorm2d(4),\n",
    "          nn.ReLU(inplace=True),\n",
    "          nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "      )\n",
    "\n",
    "      self.linear_layers = nn.Sequential(\n",
    "          nn.Linear(4 * 7 * 7, 10)\n",
    "      )\n",
    "\n",
    "  # Defining the forward pass    \n",
    "  def forward(self, x):\n",
    "      x = self.cnn_layers(x)\n",
    "      x = x.view(x.size(0), -1)\n",
    "      x = self.linear_layers(x)\n",
    "      return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let’s also define the optimizer and loss function then we will look at the summary of this model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (cnn_layers): Sequential(\n",
      "    (0): Conv2d(1, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (4): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (5): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (6): ReLU(inplace=True)\n",
      "    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (linear_layers): Sequential(\n",
      "    (0): Linear(in_features=196, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# defining the model\n",
    "model = Net()\n",
    "# defining the optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "# defining the loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# checking if GPU is available\n",
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()\n",
    "    criterion = criterion.cuda()\n",
    "    \n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we have 2 convolutional layers that will help to extract features from the images. Features from these convolutional layers are passed to the fully connected layer which classifies the images into their respective class. Now our model architecture is ready, let’s train this model for 10 epochs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Training loss: 0.16605456616046396\n",
      "Epoch 2 - Training loss: 0.09048631871148928\n",
      "Epoch 3 - Training loss: 0.07914899759141526\n",
      "Epoch 4 - Training loss: 0.07150491072596717\n",
      "Epoch 5 - Training loss: 0.06794889853261452\n",
      "Epoch 6 - Training loss: 0.06573310087049958\n",
      "Epoch 7 - Training loss: 0.06363410825184834\n",
      "Epoch 8 - Training loss: 0.061638841598502185\n",
      "Epoch 9 - Training loss: 0.05831823686533955\n",
      "Epoch 10 - Training loss: 0.05911459443050184\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    running_loss = 0\n",
    "    for images, labels in trainloader:\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "          images = images.cuda()\n",
    "          labels = labels.cuda()\n",
    "\n",
    "        # Training pass\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model(images)\n",
    "        loss = criterion(output, labels)\n",
    "        \n",
    "        #This is where the model learns by backpropagating\n",
    "        loss.backward()\n",
    "        \n",
    "        #And optimizes its weights here\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    else:\n",
    "        print(\"Epoch {} - Training loss: {}\".format(i+1, running_loss/len(trainloader)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that the training is decreasing with an increasing number of epochs. This means that our model is learning patterns from the training set. Let’s check the performance of this model on the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number Of Images Tested = 10000\n",
      "\n",
      "Model Accuracy = 0.9629\n"
     ]
    }
   ],
   "source": [
    "# getting predictions on test set and measuring the performance\n",
    "correct_count, all_count = 0, 0\n",
    "for images,labels in testloader:\n",
    "  for i in range(len(labels)):\n",
    "    if torch.cuda.is_available():\n",
    "        images = images.cuda()\n",
    "        labels = labels.cuda()\n",
    "    img = images[i].view(1, 1, 28, 28)\n",
    "    with torch.no_grad():\n",
    "        logps = model(img)\n",
    "\n",
    "    \n",
    "    ps = torch.exp(logps)\n",
    "    probab = list(ps.cpu()[0])\n",
    "    pred_label = probab.index(max(probab))\n",
    "    true_label = labels.cpu()[i]\n",
    "    if(true_label == pred_label):\n",
    "      correct_count += 1\n",
    "    all_count += 1\n",
    "\n",
    "print(\"Number Of Images Tested =\", all_count)\n",
    "print(\"\\nModel Accuracy =\", (correct_count/all_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing a CNN in TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bsram\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\bsram\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\bsram\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\bsram\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\bsram\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\bsram\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "C:\\Users\\bsram\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\bsram\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\bsram\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\bsram\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\bsram\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\bsram\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "# importing the libraries\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.14.0\n"
     ]
    }
   ],
   "source": [
    "# version of tensorflow\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we are using the 2.2.0 version of TensorFlow. Let’s now load the MNIST dataset using the datasets class of tensorflow.keras:\n",
    "\n",
    "(train_images, train_labels), (test_images, test_labels) = datasets.mnist.load_data(path='mnist.npz')\n",
    "# Normalize pixel values to be between 0 and 1\n",
    "train_images, test_images = train_images / 255.0, test_images / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 9 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# visualizing a few images\n",
    "plt.figure(figsize=(10,10))\n",
    "for i in range(9):\n",
    "    plt.subplot(3,3,i+1)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.grid(False)\n",
    "    plt.imshow(train_images[i], cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(((60000, 28, 28), (60000,)), ((10000, 28, 28), (10000,)))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# shape of the training and test set\n",
    "(train_images.shape, train_labels.shape), (test_images.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshaping the images\n",
    "train_images = train_images.reshape((60000, 28, 28, 1))\n",
    "test_images = test_images.reshape((10000, 28, 28, 1))\n",
    "\n",
    "# one hot encoding the target variable\n",
    "train_labels = to_categorical(train_labels)\n",
    "test_labels = to_categorical(test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Model Architecture\n",
    "Now, we will define the architecture of our model. We will use the same architecture which we defined in PyTorch. So, our model will have 2 convolutional layers, with a combination of max-pooling layers, then we will have a flatten layer and finally a dense layer with 10 neurons since we have 10 classes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\bsram\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# defining the model architecture\n",
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(4, (3, 3), activation='relu', input_shape=(28, 28, 1)))\n",
    "model.add(layers.MaxPooling2D((2, 2), strides=2))\n",
    "model.add(layers.Conv2D(4, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2), strides=2))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 26, 26, 4)         40        \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 13, 13, 4)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 11, 11, 4)         148       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 5, 5, 4)           0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 10)                1010      \n",
      "=================================================================\n",
      "Total params: 1,198\n",
      "Trainable params: 1,198\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Let’s quickly look at the summary of the model:\n",
    "\n",
    "# summary of the model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compiling the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 37s 609us/sample - loss: 0.4676 - acc: 0.8541 - val_loss: 0.1976 - val_acc: 0.9392\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 38s 635us/sample - loss: 0.1935 - acc: 0.9407 - val_loss: 0.1478 - val_acc: 0.9548\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 32s 538us/sample - loss: 0.1530 - acc: 0.9531 - val_loss: 0.1185 - val_acc: 0.9629\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 39s 657us/sample - loss: 0.1317 - acc: 0.9600 - val_loss: 0.1031 - val_acc: 0.9685 0\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 45s 746us/sample - loss: 0.1195 - acc: 0.9630 - val_loss: 0.0981 - val_acc: 0.9688\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 41s 678us/sample - loss: 0.1105 - acc: 0.9657 - val_loss: 0.0928 - val_acc: 0.9714\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 41s 677us/sample - loss: 0.1050 - acc: 0.9688 - val_loss: 0.0839 - val_acc: 0.9736\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 45s 757us/sample - loss: 0.0998 - acc: 0.9702 - val_loss: 0.0827 - val_acc: 0.9749\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 47s 786us/sample - loss: 0.0959 - acc: 0.9712 - val_loss: 0.0889 - val_acc: 0.9720\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 41s 691us/sample - loss: 0.0920 - acc: 0.9719 - val_loss: 0.0858 - val_acc: 0.9720\n"
     ]
    }
   ],
   "source": [
    "# training the model\n",
    "history = model.fit(train_images, train_labels, epochs=10, validation_data=(test_images, test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To summarize, initially, the training loss was about 0.46 and after 10 epochs, the training loss reduced to 0.08. The training and validation accuracies after 10 epochs are 97.31% and 97.48% respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes\n",
    "To summarize,we first looked at a brief overview of PyTorch and TensorFlow. Then we understood the MNIST handwritten digit classification challenge and finally, build an image classification model using CNN(Convolutional Neural Network) in PyTorch and TensorFlow. Now, I hope you will be familiar with both these frameworks. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
